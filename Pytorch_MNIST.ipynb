{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69bc4abb",
   "metadata": {},
   "source": [
    "# Handschriftenerkennung mit PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4038e36b",
   "metadata": {},
   "source": [
    "Zum Reinkommen und Einüben des PyTorch Frameworks habe ich die erste Programmieraufgabe, die wir noch from the scratch mit Numpy programmiert haben, mal in PyTorch umgesetzt. Am besten lernt man ein neues Framework meiner Meinung nach kennen, wenn man einfache, bekannte Ideen umsetzt. Dabei bin ich in seeeehr viele Probleme reingerannt und möchte euch mit diesem Notebook ein wenig mitnehmen, damit ihr von meinen Erfahrungen profitieren könnt. Allerdings muss ich euch ja nicht sagen, dass selbst ausprobieren mehr hilft. Daher kommt am Ende eine kleine freiwillige Aufgabe, an der ich selbst eine halbe Ewigkeit saß (die jetzt aber total trivial wirkt).\n",
    "\n",
    "Die meisten, wirklich allermeisten Probleme hatte ich, weil ich die Datentypen nicht richtig angeschaut habe. Wie die Daten strukturiert und gespeichert sind ist echt wichtig und ich gehe Stück für Stück mit euch da durch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4836af43",
   "metadata": {},
   "source": [
    "## Daten, Daten, Daten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abdad6f",
   "metadata": {},
   "source": [
    "### Daten importieren"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c7259e",
   "metadata": {},
   "source": [
    "Zunächst den ganzen Spaß importieren und die Daten herunterladen. Falls ihr die schon einmal heruntergeladen habt, könnt ihr das Argument `download=False` so stehen lassen, andernfalls muss das zu `True` geändert werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b253bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplaotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d32f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda, Compose\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc72544f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist_data(train):\n",
    "    from torchvision import datasets, transforms\n",
    "    from torch.utils.data import DataLoader\n",
    "    \n",
    "    transform = transforms.Compose([ transforms.ToTensor(),\n",
    "                                     transforms.Normalize(mean=[0.5,], std=[0.5,]) \n",
    "                                   ])\n",
    "    dataset = datasets.MNIST('./MNIST/', train=train, transform=transform,  download=False)\n",
    "    return dataset\n",
    "\n",
    "train_dataset = load_mnist_data(train=True)\n",
    "test_dataset = load_mnist_data(train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02974800",
   "metadata": {},
   "source": [
    "Mit dem `transform` Parameter können wir die Daten so vorbereiten, wie wir sie haben wollen. Wir möchten sie wie in der Aufgabe als Werte zwischen -1 und 1 haben. Problem: Sie kommen als Werte zwischen 0 und 1. Und zwar auch nicht als Tensor, sondern als `PIL`-Objekt, ein Datentyp für Bilder. \n",
    "\n",
    "Der `transform.Compose` Aufruf schaltet mehrere Transformationen hintereinander, die wir dann übergeben können. Zunächst wollen wir aus dem `PIL`-Objekt einen Tensor mit `transforms.ToTensor()` bekommen. Dann möchten wir die Daten shiften. Die Funktion `transforms.Normalize` verschiebt die Daten so, dass sie Mittelwert 0 und Varianz 1 hat. Genauer:\n",
    "Für Eingangsdaten $x \\in [0, 1]$ berechnet die Funktion \n",
    "\n",
    "$$ x^* = \\frac{x - \\text{mean}}{\\text{std}} $$\n",
    "\n",
    "Wenn das aber die Berechnung ist, sehen wir leicht, dass wir die Daten, die zwischen 0 und 1 liegen, mit `mean=0.5` und `std=0.5` auf -1 bis 1 shiften können.\n",
    "\n",
    "Die Daten werden als Liste `[0.5,]` übergeben, die die Werte für jeden Channel einzeln beinhaltet. Da wir nur einen Channel haben, brauchen wir nur ein Element.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908d2411",
   "metadata": {},
   "source": [
    "### Daten genaaaaaau anschauen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3da1ea",
   "metadata": {},
   "source": [
    "Ich kann's nicht oft genug sagen: Schaut euch die Daten genau an. Wie sind sie strukturiert? Dazu geben wir einige Dinge aus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e95a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"a:\", type(train_dataset))\n",
    "print(\"b:\", type(train_dataset[0]))\n",
    "print(\"c:\", len(train_dataset[0]))\n",
    "print(\"d:\", type(train_dataset[0][0]))\n",
    "print(\"e:\", type(train_dataset[0][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd72fa4",
   "metadata": {},
   "source": [
    "`train_dataset` ist ein Objekt vom Typ `torchvision.datasets.mnist.MNIST` (a). Das ist ein für diese speziellen Daten eigens programmierter Datentyp. Für uns ist nur wichtig, dass dieser iterable ist, wir können also über diese eine `for`-Schleifen laufen lassen und auf die Daten mit den eckigen Klammern `[]` zugreifen.\n",
    "\n",
    "Tun wir dies, bekommen wir in der ersten Dimension Tupel (b) der Länge 2 (c). Die erste Komponente dieses Tupels ist ein `torch.Tensor` (d), die zweite ist ein Integer (e).\n",
    "\n",
    "Schauen wir uns das erste Tupel mal an:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1fac54",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6b2ad6",
   "metadata": {},
   "source": [
    "Dieses Tupel besteht also aus dem Signal, gespeichert als Tensor, und dem Label. Der Tensor hat folgende Größe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd87a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_dataset[0][0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d48c3f4",
   "metadata": {},
   "source": [
    "Nun haben wir eine gute Vorstellung über unsere Daten. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bf28f0",
   "metadata": {},
   "source": [
    "## Parameter definieren und DataLoader instantiieren"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b68914",
   "metadata": {},
   "source": [
    "Der DataLoader gibt uns jeweils immer einen Batch an Daten zurück, wenn wir über diesen iterieren. Das vereinfacht unseren Code. Da wir dafür schon eine unsere Netzwerkkonfigurationen benötigen (nämlich die batch_size), definieren wir hier nun gleich alle davon auf einmal. Hier wird unser Maschinenraum sein, welchen wir zum Ausprobieren anpassen können.\n",
    "\n",
    "Damit erstellen wir uns dann unsere DataLoader..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c166e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "hidden_size = 10\n",
    "learning_rate = 0.005\n",
    "label_smoothing = 0.0\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d7693c",
   "metadata": {},
   "source": [
    "...welcher uns immer einen Batch lädt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aece7079",
   "metadata": {},
   "outputs": [],
   "source": [
    "for X, y in train_dataloader:\n",
    "    print(X.shape)\n",
    "    print(y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669e641c",
   "metadata": {},
   "source": [
    "## Neural Network definieren und trainieren\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329f8e37",
   "metadata": {},
   "source": [
    "Wir bauen uns ein NN mit einem hidden layer mit 10 Logits, da wir 10 Klassen haben. Wir verwenden den Cross Entropy Loss. Hier wieder ein Punkt zu den Daten: Der Cross Entropy Loss erhält ungleiche Daten. Die targets (echte Labels) des NN sind *keine* codierten Vektoren (One-Hot-Vektoren) sondern einfache Zahlen von 0-9. Die prediction hingegen (die Ausgabe des Netzwerks) ist ein 10-elementiger Vektor mit den Logits. Die Funktion nn.CrossEntropyLoss() nimmt diese so an und berechnet auch selbstständig den Softmax. Deshalb hat unser NN keine Softmax eingebaut, sondern berechnet nur die Logits. Siehe dazu die Vorlesungsvideos, in der Prof. Frahling diesen Punkt genauer erklärt hat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1a28c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definiere das kleine Model mit nur einem hidden layer ohne softmax\n",
    "class MyNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyNN, self).__init__()\n",
    "        self.flatten = nn.Flatten(start_dim=-3, end_dim=-1)\n",
    "        self.linear_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, hidden_size, bias=False),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        return self.linear_stack(x)\n",
    "\n",
    "# Der train_dataloader gibt uns jeweils einen batch zurück. Hier sehen wir die shapes:\n",
    "for X, y in train_dataloader:\n",
    "    print(\"Shape of X [N, C, H, W]: \", X.shape)\n",
    "    print(\"Shape of y: \", y.shape, y.dtype)\n",
    "    break\n",
    "    \n",
    "\n",
    "# Wir instantiieren das Model und weisen es ggf. der GPU zu.\n",
    "model = MyNN().to(device)\n",
    "\n",
    "# Unser CrossEntropyLoss, der Logits annimmt und den Softmax eingebaut hat\n",
    "loss_fn = nn.CrossEntropyLoss(label_smoothing=label_smoothing)\n",
    "\n",
    "# Optimizer & SummaryWriter. \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "writer = SummaryWriter(f\"zahlen_erkennen_runs/layer-1_size_{hidden_size}_bs_{batch_size}_lr_{learning_rate}_ls_{label_smoothing}\")\n",
    "\n",
    "##### Starte Training ######\n",
    "\n",
    "\"\"\"\n",
    "Zur Epochenzahl: Das haben viele von euch in Uebung 9 - zumindest in meinen Gruppen - missverstanden: Die Epochenzahl ist nicht dazu\n",
    "gedacht, so viele Epochen tatsächlich durchzulaufen. Das schafft keine Hardware in angemessener Zeit.\n",
    "Es soll das Training am laufen halten, damit ihr im Tensorboard die Performance live mitverfolgen\n",
    "und erst bei Bedarf das Training beenden können sollt. Das Tensorboard aktualisiert on the fly. Lasst\n",
    "deshalb den Wert so hoch und wenn ihr genug gesehen habt, stoppt diese Zelle manuell.\n",
    "\"\"\"\n",
    "epochs = 30000\n",
    "\n",
    "# Anzahl von einzelnen Inputs (Bildern)\n",
    "size = len(train_dataloader.dataset)\n",
    "\n",
    "# Steps = Anzahl bisheriger Batches, nicht Signale\n",
    "steps = 0\n",
    "train_loss = 0\n",
    "\n",
    "print(f\"Working on {device}\")\n",
    "# Gehen wir nun das Training durch. Anders als in der Uebung 9 bspw. habe ich das alles hier\n",
    "# reingeschrieben (also keine Aufgaben outgesourced in Funktionen)\n",
    "for t in range(epochs):\n",
    "    print(f\"\\nEpoche {t+1}\\n-------------------------------------\")\n",
    "    \n",
    "    # Starte Trainingsmodus des Models\n",
    "    model.train()\n",
    "    \n",
    "    # Anzahl korrekt berechneter Labels (für die Accuracy)\n",
    "    correct = 0\n",
    "    \n",
    "    # Gehe nun die Batches durch. Am Ende berechnen wir für jeden 10. Batch die Performance (Loss, Accuracy)\n",
    "    for batch, (X, y) in enumerate(train_dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        \n",
    "        # Das Model ist zwar nur für einzelne Signale definiert, nimmt aber auch ganze Batches auf einmal.\n",
    "        # Übrigens: Wer sich wundert, dass hier ein Objekt wie eine Funktion aufgerufen wird, siehe\n",
    "        # https://youtu.be/ytOytMdqD7Y für eine Erklärung\n",
    "        pred = model(X)\n",
    "        \n",
    "        # Achtung: Hier passiert das, wovon ich oben geredet habe. Die predictions haben\n",
    "        # eine Shape von (batch_size, 10), die targets y haben shape (batch_size). Die\n",
    "        # Funktion CrossEntropyLoss() kann damit umgehen.\n",
    "        loss = loss_fn(pred, y)\n",
    "        \n",
    "        # Setze Gradienten auf 0 (damit sie sich nicht aufsummieren)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Berechne alle Gradienten\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update die Variablen mit den Gradienten\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Für jedes 10. Batch berechnen wir ...\n",
    "        if batch % 10 == 0:\n",
    "            \n",
    "            # ...den Loss und geben sie mit der aktuellen Anzahl an berechneten Daten aus.\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "            \n",
    "            writer.add_scalar('Loss/train', loss, steps+batch)\n",
    "            \n",
    "        # Wir berechnen, wie viele der Daten richtig vorausgesagt wurden. Die predictions sind\n",
    "        # weiterhin eine (batch_size, 10)-geshapte Liste. pred.argmax(1) gibt den Index des größten Eintrags\n",
    "        # (= der größten Wahrscheinlichkeit) für die erste Dimension (= horizontale Achse, als Matrix betrachtet)\n",
    "        # aus. Das wird für alle Einträge gemacht und dies gibt einen Vektor der Länge batch_size mit Einträgen\n",
    "        # True bzw. False aus, wobei der Wert jeweils dafür steht, ob die maximale Wahrscheinlichkeit dem richtigen\n",
    "        # Index zugewiesen wurde. type(..) rechnet sie zu floats um (True=1.0, False=0.0) und sum() und item() geben\n",
    "        # die Summe als built-in Datentyp float aus.\n",
    "        correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "        train_loss+=loss\n",
    "    \n",
    "    # Nun haben wir eine Epoche durchgerechnet und mitteln die Werte. Der train_loss wird\n",
    "    # über die Anzahl der batches (len(train_dataloader)) gemittelt, der wurde für jeden\n",
    "    # Batch aufsummiert. correct, also die Anzahl richtig vorhergesagter Signale, haben \n",
    "    # wir für jedes einzelne Signal berechnet, teilen also durch die Anzahl der Daten \n",
    "    # insgesamt (durch len(train_dataloader.dataset))\n",
    "    train_loss = train_loss / len(train_dataloader) \n",
    "    correct = correct / len(train_dataloader.dataset)\n",
    "    \n",
    "    # Anzahl aller berechneten batches\n",
    "    steps += batch\n",
    "    \n",
    "    writer.add_scalar('Accuracy/train', correct*100.0, steps)\n",
    "    \n",
    "    # Wir berechnen die Accuracy für den train loss in jeder epoch ein mal\n",
    "    print(f\"Train Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {train_loss:>8f} \\n\") \n",
    "\n",
    "    \n",
    "    #... und das gleiche noch mal in jeder Epoche einmal für die Testdaten. Vorher setzen wir\n",
    "    # das Model auf den evaluation mode. Die \"with torch.no_grad():\"-Umgebung verhindert eine\n",
    "    # teure Gradientenberechnung, die wir eh nicht brauchen.\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item() # summiert richtige vorhersagen auf\n",
    "    test_loss = test_loss / len(test_dataloader) # durchschnittlicher loss auf den batches\n",
    "    correct = correct / len(test_dataloader.dataset) # zwischen 0 und 1, accuracy\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")            \n",
    "    writer.add_scalar('Loss/test', test_loss, steps)\n",
    "    writer.add_scalar('Accuracy/test', 100.0*correct, steps)\n",
    "\n",
    "writer.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f6a229",
   "metadata": {},
   "source": [
    "Dieser Code schreibt auch Tensorboard-Daten. Schaut sie euch mal an! Geht dazu mit der Anaconda Shell in den Ordner `runs` (mit dem Befehl `cd`) und führt `tensorboard --logdir=.` aus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060889f5",
   "metadata": {},
   "source": [
    "## Unsere Variablen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b683f8",
   "metadata": {},
   "source": [
    "Wir haben als Variable nur diese eine Matrix ohne Bias. Besonders interessant finde ich, wie diese nun aussehen. Da die erste Matrix direkt in die Logits rechnet, muss diese, damit der erste Logit bei einer 1 besonders hoch ist, genau an den Pixeln einen hohen Wert haben, damit sie sich mit der 1 zu einem hohen Skalarprodukt berechnet. Wir können jetzt diese Matrix mal als Bild interpretieren und schauen, wie sie aussehen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f496e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_data_as_images(data,labels=None):\n",
    "    \"\"\"Diese Funktion kann genutzt werden, um Daten als Bild darzustellen.\"\"\"\n",
    "    if len(data.shape)==3:\n",
    "        #Several images - output them as an array\n",
    "        cols = int(np.sqrt(data.shape[0]))\n",
    "        rows = (data.shape[0]+cols-1)//cols\n",
    "        figsize = 15\n",
    "        if cols>5:\n",
    "            figsize=3*cols\n",
    "        fig = plt.figure(figsize=(figsize, figsize))  # width, height in inches\n",
    "        if rows>5:\n",
    "            fig.subplots_adjust(top = 1.0)\n",
    "        for i,image in enumerate(data):\n",
    "            sub = fig.add_subplot(cols, rows, i + 1)\n",
    "            if labels is not None:\n",
    "                sub.title.set_text('Label '+str(labels[i]))\n",
    "            sub.imshow(image,vmin=-1, vmax=1, cmap='Greys')\n",
    "        #plt.imshow(fig)\n",
    "        plt.show()\n",
    "    else:\n",
    "        if labels is not None:\n",
    "            print(\"Label:\",labels)\n",
    "        plt.imshow(data,vmin=-1, vmax=1, cmap='Greys')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc74d9a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for p in model.parameters():\n",
    "    show_data_as_images(p.cpu().detach().view(-1,28,28))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1754f15",
   "metadata": {},
   "source": [
    "Wenn ihr das lange genug laufen lassen habt, seht ihr jetzt Bilder, die wie eine Mondlandschaft mit Kratern aussieht. Legt man ein Objekt - eine Zahl - oben drüber, ist es genau dann besonders hoch, wenn diese Zahl gut in diesen Krater 'reinfällt'. Das find ich echt am interessantesten."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4dad689",
   "metadata": {},
   "source": [
    "## Now it's your turn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b4e150",
   "metadata": {},
   "source": [
    "Ich habe dieses Notebook geschrieben, um an einem einfachen Beispiel, das wir bereits kennen, PyTorch besser zu verstehen. Mein Appell ist, dass ihr dieses Notebook genau durchgeht und jeden einzelnen Schritt verstehen sollt, um dann kompliziertere Netzwerke verstehen/selbst bauen zu können. Spielt gerne etwas rum! Falls ihr was nicht versteht, schreibt mir. \n",
    "\n",
    "Als kleine Anregung: Versucht mal, den Loss zu ändern. Zum Beispiel zum mean squared error ( nn.MSELoss() ). Dieser hat keinen Softmax eingebaut! Überlegt, wo ihr diese einbauen müsst & vorallem, wie ihr die Labels transfomieren müsst, damit diese für den Loss die richtige Form haben. Tipp: Sucht nach One-Hot.\n",
    "\n",
    "Eine Mailadresse auf der ihr mich erreichen könnt ist auf meinem GitHub Profil angegeben.\n",
    "\n",
    "Viel Spaß!\n",
    "Juan"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "206.917px",
    "left": "1709.58px",
    "right": "20px",
    "top": "155px",
    "width": "372.833px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
